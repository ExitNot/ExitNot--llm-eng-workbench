{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55dfe132",
   "metadata": {},
   "source": [
    "# Meeting summary pipeline\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Here we will try to implement meeting summary pipeline espesialy audio to text summary.\n",
    "\n",
    "Recording is taken from this [dataset](https://huggingface.co/datasets/huuuyeah/meetingbank)\n",
    "Audio files can be found [here](https://huggingface.co/datasets/huuuyeah/MeetingBank_Audio/tree/main)\n",
    "\n",
    "For testing purpose audio file is taken localy into '/extra' folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f77d33c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, AutoTokenizer, AutoModelForCausalLM, pipeline, TextStreamer, QuantoConfig\n",
    "import numpy as np\n",
    "from IPython.display import display, Markdown\n",
    "import torch\n",
    "from typing import Optional\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68567e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv() \n",
    "\n",
    "# Add project root to Python path and change working directory\n",
    "project_root = Path().resolve().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "os.chdir(project_root)  # Change working directory to project root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14ad45ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "# Sign in to HuggingFace Hub\n",
    "\n",
    "hf_token = os.environ.get('HF_TOKEN')\n",
    "if hf_token is None:\n",
    "    raise ValueError(\"HF_TOKEN is not set\")\n",
    "login(hf_token, add_to_git_credential=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d1e4eb",
   "metadata": {},
   "source": [
    "Brifly about models we are using here:\n",
    "\n",
    "**distil-medium.en** - is a distilated whisper model that is trained using Robust Knowledge Distillation. Distilated model will be not only smaller by 49% but also 6 times faster.\n",
    "- More about Robust Knowledge Distillation you can read [here](https://arxiv.org/abs/2311.00430)\n",
    "- Model can be found [here](https://huggingface.co/distil-whisper/distil-medium.en)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a476011b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "ASR_MODEL = \"distil-whisper/distil-medium.en\"\n",
    "\n",
    "audio_file_name = \"extra/denver_meeting_rec.mp3\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab7c064",
   "metadata": {},
   "source": [
    "## Audio To Text\n",
    "\n",
    "Let's start from our ASR. We will use distil-whisper using HF transformers pipelines. As long as our audio is longer than 30-seconds (when *Short-Form Transcription* can be used) we will use *Long-Form Transcription*. \n",
    "\n",
    "Small note from model page:\n",
    "> Distil-Whisper uses a chunked algorithm to transcribe long-form audio files (> 30-seconds).\n",
    "> In practice, this chunked long-form algorithm is 9x faster than the sequential > algorithm proposed by OpenAI in the Whisper paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38934ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Device set to use cpu\n",
      "Using `chunk_length_s` is very experimental with seq2seq models. The results will not necessarily be entirely accurate and will have caveats. More information: https://github.com/huggingface/transformers/pull/20104. Ignore this warning with pipeline(..., ignore_warning=True). To use Whisper for long-form transcription, use rather the model's `generate` method directly as the model relies on it's own chunking mechanism (cf. Whisper original paper, section 3.8. Long-form Transcription).\n"
     ]
    }
   ],
   "source": [
    "# Initialize ASR model\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    ASR_MODEL, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(ASR_MODEL)\n",
    "\n",
    "asr_pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    max_new_tokens=128,\n",
    "    chunk_length_s=15, # This will enable Long-Form Transcription\n",
    "    # batch_size=16, # for batch processing\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f28d848",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " kind of the confluence of this whole idea of the confluence week, the merging of two rivers, and as we've kind of seen recently in politics and in the world, there's a lot of situations where water is very important right now and it's a very big issue so that is the reason that the back of the logo is considered water. So let you see the reason behind the logo and all the meanings behind the symbolism. And you'll hear a little bit more about our Confluence Week is basically highlighting all of these indigenous events and things that are happening around Denver so that we can kind of bring more people together and kind of share this whole idea of Indigenous Peoples Day. So thank you. Thank you so much and thanks for your leadership. All right. Welcome to the Denver City Council meeting of Monday, October 9th. Please rise with the Pledge of Allegiance by Councilman Lopez. I pledge allegiance to the flag of the United States of America, to the Republic of which it stands, one nation unde"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "transcription = asr_pipe(audio_file_name)\n",
    "display(Markdown(transcription['text'][:1000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64590cbc",
   "metadata": {},
   "source": [
    "## Summarization\n",
    "\n",
    "Now when ASR is ready we can start working on summarization part. \n",
    "One of the important points of this project is to try [quantization](https://huggingface.co/docs/optimum/en/concept_guides/quantization), so I will try 2 aproaches \n",
    " - Use 8b parameter model with 4bit quantization. (**meta-llama/Meta-Llama-3.1-8B-Instruct**)\n",
    " - Use model that specialized on summarizations. (**sshleifer/distilbart-cnn-12-6**)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c090c5f",
   "metadata": {},
   "source": [
    "### LLAMA_3_1\n",
    "\n",
    "Here I will try the power of 8B pamrameters version of model Meta-Llama-3.1. \n",
    "This model is huge with FP16 weights ~16 GB. But there is a way to handle this and it is a quantization.\n",
    "\n",
    "Quantisation:\n",
    "> reduce the computational and memory costs of running inference by representing the weights and activations with low-precision data types\n",
    "\n",
    "Course suggestion is to use BitsAndBytes, but it is not available for Mac with Apple Silicon chips and I decide to change quantization method. I will try:\n",
    " - optimum-quanto to int8\n",
    " - GGUF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b369cb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model of choice\n",
    "LLAMA_3_1 = \"meta-llama/Meta-Llama-3.1-8B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "aef4278b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompts\n",
    "default_system_message = \"You are an assistant that produces minutes of meetings from transcripts, with summary, key discussion points, takeaways and action items with owners, in markdown.\"\n",
    "default_user_prompt = \"Below is an extract transcript of council meeting. Please write minutes in markdown, including a summary with attendees, location and date; discussion points; takeaways; and action items with owners.\\n{transcription}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf7d638",
   "metadata": {},
   "source": [
    "#### optimum-quanto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b314a440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurations\n",
    "\n",
    "# Mac with M1/M2/M3 chip will not BitsAndBytesConfig so I will try another quantization method, but left another aproach for you\n",
    "# default_4b_quant_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,  # Use 4-bit quantization\n",
    "#     bnb_4bit_use_double_quant=True,  # Use double quantization\n",
    "#     bnb_4bit_compute_dtype=torch.bfloat16,  # Use bfloat16 for computation\n",
    "#     bnb_4bit_quant_type=\"nf4\"  # Use NF4 quantization type (Not supported on MAC)\n",
    "# )\n",
    "\n",
    "# Let's try optimum-quanto\n",
    "default_optimum_quant_config = QuantoConfig(weights=\"int8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "66edf747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Util functions\n",
    "def build_messages(system: str, user: str) -> list[dict]:\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system},\n",
    "        {\"role\": \"user\", \"content\": user}\n",
    "    ]\n",
    "\n",
    "# Summarizer class\n",
    "class MeetingSummarizer:\n",
    "    model_name: str\n",
    "    quant_config: QuantoConfig\n",
    "    tokenizer: AutoTokenizer\n",
    "    model: AutoModelForCausalLM\n",
    "    system_message: str = default_system_message\n",
    "    user_prompt: str = default_user_prompt\n",
    "    device: str\n",
    "\n",
    "    def __init__(self, model_name: str, quant_config: QuantoConfig,\n",
    "                 system_message: Optional[str] = None, user_prompt: Optional[str] = None,\n",
    "                 device: str = \"mps\"):\n",
    "        self.model_name = model_name\n",
    "        self.quant_config = quant_config\n",
    "        self.device = device\n",
    "        \n",
    "        # Initialize tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        self.tokenizer = tokenizer\n",
    "        if tokenizer is None:\n",
    "            raise ValueError(\"Tokenizer is not provided\")\n",
    "        \n",
    "        # Initialize model with proper device handling\n",
    "        try:\n",
    "            # Try with device_map=\"auto\" first (requires accelerate)\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name, \n",
    "                device_map=\"auto\", \n",
    "                quantization_config=quant_config,\n",
    "                torch_dtype=torch.float16 if device != \"cpu\" else torch.float32\n",
    "            )\n",
    "            print(f\"‚úÖ Model loaded with device_map='auto'\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  device_map='auto' failed: {e}\")\n",
    "            print(\"üîÑ Falling back to manual device placement...\")\n",
    "            \n",
    "            # Fallback: load without device_map and manually move to device\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name, \n",
    "                quantization_config=quant_config,\n",
    "                torch_dtype=torch.float16 if device != \"cpu\" else torch.float32,\n",
    "                low_cpu_mem_usage=True\n",
    "            )\n",
    "            if device != \"cpu\":\n",
    "                self.model = self.model.to(device)\n",
    "            print(f\"‚úÖ Model loaded and moved to {device}\")\n",
    "        \n",
    "        # Set custom messages if provided\n",
    "        if system_message is not None:\n",
    "            self.system_message = system_message\n",
    "        if user_prompt is not None:\n",
    "            self.user_prompt = user_prompt\n",
    "        \n",
    "    def summarize(self, transcription: str) -> str:\n",
    "        # Check if this is a chat model (like Llama) or a summarization model (like DistilBART)\n",
    "        is_chat_model = hasattr(self.tokenizer, 'chat_template') and self.tokenizer.chat_template is not None\n",
    "        \n",
    "        if is_chat_model:\n",
    "            # For chat models like Llama, use chat template\n",
    "            formatted_prompt = self.user_prompt.format(transcription=transcription)\n",
    "            messages = build_messages(self.system_message, formatted_prompt)\n",
    "            inputs = self.tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(self.device)\n",
    "        else:\n",
    "            # For summarization models like DistilBART, use direct text input\n",
    "            # For BART models, we typically just pass the text directly\n",
    "            tokenized = self.tokenizer(transcription, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "            inputs = tokenized.input_ids.to(self.device)\n",
    "            \n",
    "            # Ensure batch size is 1 for TextStreamer compatibility\n",
    "            if inputs.dim() == 1:\n",
    "                inputs = inputs.unsqueeze(0)\n",
    "        \n",
    "        # Create streamer for real-time output (only for chat models to avoid batch size issues)\n",
    "        streamer = TextStreamer(self.tokenizer, skip_prompt=True) if is_chat_model else None\n",
    "        \n",
    "        # Generate with streaming\n",
    "        with torch.no_grad():\n",
    "            if is_chat_model:\n",
    "                # Chat model generation parameters\n",
    "                outputs = self.model.generate(\n",
    "                    inputs, \n",
    "                    max_new_tokens=2000,\n",
    "                    do_sample=True,\n",
    "                    temperature=0.7,\n",
    "                    top_p=0.9,\n",
    "                    streamer=streamer,\n",
    "                    pad_token_id=self.tokenizer.eos_token_id,\n",
    "                    eos_token_id=self.tokenizer.eos_token_id \n",
    "                )\n",
    "            else:\n",
    "                # Summarization model generation parameters (without streamer to avoid batch issues)\n",
    "                outputs = self.model.generate(\n",
    "                    inputs,\n",
    "                    do_sample=True,\n",
    "                    temperature=0.7,\n",
    "                    top_p=0.9,\n",
    "                    streamer=streamer,\n",
    "                    pad_token_id=self.tokenizer.eos_token_id,\n",
    "                    eos_token_id=self.tokenizer.eos_token_id,\n",
    "                    early_stopping=True\n",
    "                )\n",
    "        \n",
    "        # Decode the full response (excluding the input prompt)\n",
    "        response = self.tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)\n",
    "        return response\n",
    "    \n",
    "    def summarize_with_display(self, transcription: str) -> str:\n",
    "        \"\"\"Summarize and display the result in Markdown format\"\"\"\n",
    "        print(\"üéØ Generating meeting summary...\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        summary = self.summarize(transcription)\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\"‚úÖ Summary generated! Displaying in Markdown:\")\n",
    "        display(Markdown(summary))\n",
    "        \n",
    "        return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310c1bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage of usage quantized model by optimum-quanto\n",
    "\n",
    "# Initialize the summarizer \n",
    "summarizer = MeetingSummarizer(\n",
    "    model_name=LLAMA_3_1,\n",
    "    quant_config=default_optimum_quant_config,\n",
    "    device=\"mps\"  # let's try Metal Performance Shaders on Mac\n",
    ")\n",
    "\n",
    "# Example: Use with transcription\n",
    "# Uncomment the following lines when you have a transcription ready:\n",
    "summary = summarizer.summarize_with_display(transcription['text'])\n",
    "\n",
    "print(\"‚úÖ MeetingSummarizer initialized with 8-bit quantization!\")\n",
    "print(\"üìù Ready to process meeting transcriptions with streaming output and Markdown display.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76db6fe1",
   "metadata": {},
   "source": [
    "**Result**: Even with such quantization works really long and will be tested seperately in async way..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0836dcf",
   "metadata": {},
   "source": [
    "#### GGUF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99daa7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed2e182",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### DESTIL BART\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2a6221",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2658 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "DISTILBART = \"sshleifer/distilbart-cnn-12-6\"\n",
    "# Example usage of usage of specialised destilated summarization model. \n",
    "# It is not chat-aware so we can inject our system and user prompts as text\n",
    "input_text = f\"\"\"\n",
    "System: {default_system_message}\n",
    "User: {default_user_prompt}\n",
    "{transcription['text']}\n",
    "\"\"\"\n",
    "\n",
    "# Here we can just use summarization pipeline \n",
    "summarizer = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\")\n",
    "summary = summarizer(\n",
    "    input_text,\n",
    "    tokenizer=\"sshleifer/distilbart-cnn-12-6\",\n",
    "    max_length=2000,     # maximum tokens in summary\n",
    "    do_sample=False,   # True ‚Üí more diverse output, False ‚Üí deterministic\n",
    "    num_beams=4        # beam search size (higher ‚Üí better quality, slower)\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1608ca8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Checking transcription length...\n",
      "üìà Transcription stats:\n",
      "   ‚Ä¢ Word count: 2,202\n",
      "   ‚Ä¢ Estimated tokens: 1,652\n",
      "   ‚Ä¢ DistilBART max context: ~1024 tokens\n",
      "‚ö†Ô∏è  Text exceeds recommended chunk size - chunking will be used\n"
     ]
    }
   ],
   "source": [
    "# Example: Using chunking with DistilBART for long transcriptions\n",
    "print(\"üìä Checking transcription length...\")\n",
    "transcription_text = transcription['text']\n",
    "word_count = len(transcription_text.split())\n",
    "estimated_tokens = word_count * 0.75\n",
    "\n",
    "print(f\"üìà Transcription stats:\")\n",
    "print(f\"   ‚Ä¢ Word count: {word_count:,}\")\n",
    "print(f\"   ‚Ä¢ Estimated tokens: {estimated_tokens:,.0f}\")\n",
    "print(f\"   ‚Ä¢ DistilBART max context: ~1024 tokens\")\n",
    "\n",
    "if estimated_tokens > 800:\n",
    "    print(\"‚ö†Ô∏è  Text exceeds recommended chunk size - chunking will be used\")\n",
    "else:\n",
    "    print(\"‚úÖ Text fits within context window\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5938f63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Testing FIXED simple pipeline with chunking...\n",
      "============================================================\n",
      "üìè System/User prompts use ~42 tokens\n",
      "üìä Available tokens for content: 508\n",
      "üìä Transcription length: 1652 estimated tokens\n",
      "üìè DistilBART context limit: 1024 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è  Text too long! Using chunking strategy...\n",
      "üìÑ Split into 5 chunks (max 406 tokens each + prompts)\n",
      "üîÑ Processing chunk 1/5...\n",
      "   üìè Chunk 1 length: 437 tokens\n",
      "‚úÖ Chunk 1 done\n",
      "üîÑ Processing chunk 2/5...\n",
      "   üìè Chunk 2 length: 437 tokens\n",
      "‚úÖ Chunk 2 done\n",
      "üîÑ Processing chunk 3/5...\n",
      "   üìè Chunk 3 length: 437 tokens\n",
      "‚úÖ Chunk 3 done\n",
      "üîÑ Processing chunk 4/5...\n",
      "   üìè Chunk 4 length: 437 tokens\n",
      "‚úÖ Chunk 4 done\n",
      "üîÑ Processing chunk 5/5...\n",
      "   üìè Chunk 5 length: 308 tokens\n",
      "‚úÖ Chunk 5 done\n",
      "üîó Combining chunk summaries...\n",
      "\n",
      "üìã FIXED Pipeline Result:\n",
      "========================================\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Section 1:  The back of the logo of the Denver City Council logo is considered water . Councilor Clark invites everyone down to the first ever Halloween parade on Broadway in Lucky District 7 . Proclamation number 17 is an observance of the second annual Indigenous peoples day in the city .\n",
       "\n",
       "Section 2:  The council of the city and county of Denver recognizes that the indigenous peoples have lived and flourished on the lands known as the America since time and memorial . Denver and surrounding communities are built upon the ancestral homelands of numerous indigenous tribes, which include the southern Ute, the Ute Mountains, Ute tribes of Colorado .\n",
       "\n",
       "Section 3:  Indigenous Indigenous Peoples Day is celebrated in Denver, Colorado, for the second time . Mayor: \"We are celebrating indigenous people's day out of pride for who we are\" Council member: \"It's very important to be proud of who you're from\"\n",
       "\n",
       "Section 4:  Councilwoman Martega: \"This day is not a day off, it's a day on in Denver, right? And addressing those critical issues\" Councilwoman Caniche: \"I'm very proud of today. Oh, and we made Time magazine and Newsweek once again today as a leader in terms of the cities that are celebrating indigenous peoples\"\n",
       "\n",
       "Section 5:  Councilwoman Artega: \"I just wanted to say thank you many of the Native American peoples of Colorado have been at the forefront or actually nationally of defending some of the public lands that have been protected over the last few years that are under attack right now. And there are places that the communities have fought to protect, but that everyone gets to enjoy\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def chunk_text_simple(text: str, max_chunk_size: int = 800, overlap_size: int = 100) -> list[str]:\n",
    "    \"\"\"\n",
    "    Split text into overlapping chunks suitable for model processing.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text to chunk\n",
    "        max_chunk_size: Maximum tokens per chunk (conservative estimate using words)\n",
    "        overlap_size: Number of tokens to overlap between chunks\n",
    "    \n",
    "    Returns:\n",
    "        List of text chunks\n",
    "    \"\"\"\n",
    "    # Simple word-based chunking (rough token estimation: ~1.3 words per token)\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    \n",
    "    # Convert token estimates to word counts\n",
    "    max_words = int(max_chunk_size * 1.3)  # Conservative estimate\n",
    "    overlap_words = int(overlap_size * 1.3)\n",
    "    \n",
    "    start = 0\n",
    "    while start < len(words):\n",
    "        # Calculate end position\n",
    "        end = min(start + max_words, len(words))\n",
    "        \n",
    "        # Create chunk\n",
    "        chunk = ' '.join(words[start:end])\n",
    "        chunks.append(chunk)\n",
    "        \n",
    "        # Move start position (with overlap)\n",
    "        if end >= len(words):\n",
    "            break\n",
    "        start = end - overlap_words\n",
    "        \n",
    "        # Ensure we make progress\n",
    "        if start <= 0:\n",
    "            start = max_words\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def simple_pipeline_with_chunking(transcription_text, max_tokens=600):  # Reduced from 800\n",
    "    \"\"\"\n",
    "    Simple approach using HuggingFace pipeline with automatic chunking\n",
    "    \"\"\"\n",
    "    from transformers import pipeline\n",
    "    \n",
    "    # Calculate system/user prompt overhead\n",
    "    prompt_overhead = f\"\"\"\n",
    "        System: {default_system_message}\n",
    "        User: {default_user_prompt}\n",
    "        \"\"\"\n",
    "    prompt_tokens = len(prompt_overhead.split()) * 0.75\n",
    "    print(f\"üìè System/User prompts use ~{prompt_tokens:.0f} tokens\")\n",
    "    \n",
    "    # Adjust available space for transcription content\n",
    "    available_tokens = max_tokens - prompt_tokens - 50  # 50 token safety buffer\n",
    "    print(f\"üìä Available tokens for content: {available_tokens:.0f}\")\n",
    "    \n",
    "    # Check transcription length\n",
    "    estimated_tokens = len(transcription_text.split()) * 0.75\n",
    "    print(f\"üìä Transcription length: {estimated_tokens:.0f} estimated tokens\")\n",
    "    print(f\"üìè DistilBART context limit: 1024 tokens\")\n",
    "    \n",
    "    # Initialize pipeline\n",
    "    summarizer = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\")\n",
    "    \n",
    "    if estimated_tokens <= available_tokens:\n",
    "        print(\"‚úÖ Text fits in available space, processing directly...\")\n",
    "        \n",
    "        # Create input with system and user prompts (original approach)\n",
    "        input_text = f\"\"\"\n",
    "            System: {default_system_message}\n",
    "            User: {default_user_prompt}\n",
    "            {transcription_text}\n",
    "            \"\"\"\n",
    "        \n",
    "        # Double-check final length\n",
    "        final_tokens = len(input_text.split()) * 0.75\n",
    "        print(f\"üîç Final input length: {final_tokens:.0f} tokens\")\n",
    "        \n",
    "        if final_tokens > 1000:  # Conservative limit\n",
    "            print(\"‚ö†Ô∏è  Still too long, forcing chunking...\")\n",
    "        else:\n",
    "            result = summarizer(\n",
    "                input_text,\n",
    "                max_length=512,\n",
    "                min_length=50,\n",
    "                do_sample=False,\n",
    "                num_beams=4\n",
    "            )\n",
    "            return result[0]['summary_text']\n",
    "    \n",
    "    print(\"‚ö†Ô∏è  Text too long! Using chunking strategy...\")\n",
    "    \n",
    "    # Use much smaller chunks to account for prompt overhead\n",
    "    chunk_size = max(200, int(available_tokens * 0.8))  # Very conservative\n",
    "    chunks = chunk_text_simple(transcription_text, max_chunk_size=chunk_size, overlap_size=50)\n",
    "    print(f\"üìÑ Split into {len(chunks)} chunks (max {chunk_size} tokens each + prompts)\")\n",
    "    \n",
    "    # Summarize each chunk\n",
    "    chunk_summaries = []\n",
    "    for i, chunk in enumerate(chunks, 1):\n",
    "        print(f\"üîÑ Processing chunk {i}/{len(chunks)}...\")\n",
    "        \n",
    "        # Add system/user prompts to each chunk\n",
    "        chunk_input = f\"\"\"\n",
    "            System: {default_system_message}\n",
    "            User: {default_user_prompt}\n",
    "            {chunk}\n",
    "            \"\"\"\n",
    "        \n",
    "        # Verify chunk length before processing\n",
    "        chunk_tokens = len(chunk_input.split()) * 0.75\n",
    "        print(f\"   üìè Chunk {i} length: {chunk_tokens:.0f} tokens\")\n",
    "        \n",
    "        if chunk_tokens > 1000:\n",
    "            print(f\"   ‚ö†Ô∏è  Chunk {i} still too long! Truncating...\")\n",
    "            # Truncate the chunk content (keep prompts, cut transcription)\n",
    "            words = chunk.split()\n",
    "            max_chunk_words = int((1000 - prompt_tokens - 50) * 1.3)  # Conservative\n",
    "            truncated_chunk = ' '.join(words[:max_chunk_words])\n",
    "            chunk_input = f\"\"\"\n",
    "                System: {default_system_message}\n",
    "                User: {default_user_prompt}\n",
    "                {truncated_chunk}\n",
    "                \"\"\"\n",
    "            print(f\"   ‚úÇÔ∏è  Truncated to ~{len(chunk_input.split()) * 0.75:.0f} tokens\")\n",
    "        \n",
    "        result = summarizer(\n",
    "            chunk_input,\n",
    "            max_length=150,  # Shorter for chunks\n",
    "            min_length=30,\n",
    "            do_sample=False,\n",
    "            num_beams=4\n",
    "        )\n",
    "        \n",
    "        chunk_summaries.append(result[0]['summary_text'])\n",
    "        print(f\"‚úÖ Chunk {i} done\")\n",
    "    \n",
    "    # Combine results\n",
    "    if len(chunk_summaries) == 1:\n",
    "        return chunk_summaries[0]\n",
    "    \n",
    "    print(\"üîó Combining chunk summaries...\")\n",
    "    combined = \"\\n\\n\".join([f\"Section {i+1}: {summary}\" for i, summary in enumerate(chunk_summaries)])\n",
    "    \n",
    "    # If combined result is still too long, summarize it\n",
    "    if len(combined.split()) * 0.75 > max_tokens:\n",
    "        print(\"üìã Final summary step...\")\n",
    "        final_result = summarizer(\n",
    "            f\"Summarize this meeting: {combined}\",\n",
    "            max_length=512,\n",
    "            min_length=100,\n",
    "            do_sample=False,\n",
    "            num_beams=4\n",
    "        )\n",
    "        return final_result[0]['summary_text']\n",
    "    \n",
    "    return combined\n",
    "\n",
    "print(\"üîß Testing simple pipeline with chunking...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "fixed_summary = simple_pipeline_with_chunking(transcription['text'])\n",
    "\n",
    "print(\"\\nüìã Pipeline Result:\")\n",
    "print(\"=\" * 40)\n",
    "display(Markdown(fixed_summary))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b84b4d",
   "metadata": {},
   "source": [
    "This aproach is much faster but quolity have to be enhanced. \n",
    "Possibly one more aggregation is needed for combining chunks summaries into one summary, but reusing same model seems bad aproach, and needs to be agregated by other model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1ad0ca",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Results from UI test:\n",
    "\n",
    "DistilBart results:\n",
    "![distilbart result](distilbart-result.png)\n",
    "\n",
    "Llama3.1 8B with optimum-quanto:\n",
    "![distilbart result](llama-quanto-result.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5a2221",
   "metadata": {},
   "source": [
    "From result's it is visible how significantly large model cope with this task better even with quantization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
